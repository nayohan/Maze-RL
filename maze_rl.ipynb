{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Maze():\n",
    "    def __init__(self, row=10, col=10, epsilon=0.1, gamma=0.5, lr=1, episode_len=1000, mode=\"Q-Learning\"):\n",
    "        self.row = row\n",
    "        self.col = col\n",
    "        self.Qvalue=np.zeros((row, col, 4))\n",
    "        self.actions={\"up\":0,\"down\":1,\"right\":2,\"left\":3}\n",
    "        self.epsilon=epsilon\n",
    "        self.gamma=gamma\n",
    "        self.lr=lr\n",
    "        self.episode_len=episode_len\n",
    "        self.mode= mode\n",
    "        \n",
    "        self.maze_frame=np.array([\n",
    "            [1,0,0,0,0,0,1,1,1,1],\n",
    "            [1,1,1,1,1,0,1,0,1,1],\n",
    "            [1,1,1,0,1,0,1,1,1,1],\n",
    "            [1,0,1,0,1,0,1,0,0,1],\n",
    "            [1,0,1,0,1,0,1,0,1,1],\n",
    "            [1,0,0,0,1,0,1,0,1,0],\n",
    "            [1,0,1,1,1,0,1,0,1,0],\n",
    "            [1,0,1,0,0,0,1,0,1,1],\n",
    "            [0,1,1,1,1,1,1,0,1,1],\n",
    "            [1,1,1,1,0,0,0,0,1,1],\n",
    "        ], dtype=np.float32)\n",
    "\n",
    "    def _done(self, state_row, state_col): # 종료함수\n",
    "        if (9,9) == (state_row, state_col):\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "        \n",
    "    def _get_eps_greedy_action(self, state_row, state_col, epsilon): # 행동을 선택해주는 함수\n",
    "        if np.random.rand() < epsilon:\n",
    "            return np.random.randint(0,4)\n",
    "        else:\n",
    "            return np.nanargmax(self.Qvalue[state_row][state_col]) # 가장 높은 확률의 경로\n",
    "\n",
    "    def _get_step_reward(self, state_row, state_col, action): # 행동에 따른 reward를 찾아주는 함수\n",
    "        next_state_row, next_state_col, reward = 0, 0, 0\n",
    "        if action==0: # up\n",
    "            next_state_row, next_state_col = state_row - 1, state_col\n",
    "        elif action==1: # down\n",
    "            next_state_row, next_state_col = state_row + 1, state_col\n",
    "        elif action==2: # right\n",
    "            next_state_row, next_state_col = state_row, state_col + 1\n",
    "        elif action==3: # left\n",
    "            next_state_row, next_state_col = state_row, state_col - 1\n",
    "            \n",
    "        if 0<=next_state_row and next_state_row<self.row and 0<=next_state_col and next_state_col<self.col: # map\n",
    "            if self.maze_frame[next_state_row][next_state_col] == 0: # wall\n",
    "                next_state_row, next_state_col = state_row, state_col\n",
    "                reward = -10\n",
    "            elif self.maze_frame[next_state_row][next_state_col] == 1: # aisle\n",
    "                reward = -1\n",
    "            else:\n",
    "                print('Error:_get_step_reward', self.maze_frame[next_state_row][next_state_col])\n",
    "        else: # out of map\n",
    "            next_state_row, next_state_col = state_row, state_col\n",
    "            reward = -100\n",
    "        return next_state_row, next_state_col, reward\n",
    "        \n",
    "    def _update_Q(self, state_row, state_col, action, reward, next_state_row, next_state_col, next_action=None): \n",
    "        if self.mode==\"Q-Learning\": # 큐러닝\n",
    "            self.Qvalue[state_row][state_col][action] += self.lr * (reward + self.gamma * np.max(self.Qvalue[next_state_row][next_state_col]) - self.Qvalue[state_row][state_col][action])\n",
    "        elif self.mode==\"Sarsa\": # SARSA\n",
    "            self.Qvalue[state_row][state_col][action] += self.lr * (reward + self.gamma * self.Qvalue[next_state_row][next_state_col][next_action] - self.Qvalue[state_row][state_col][action])\n",
    "        \n",
    "    def _episode(self, start_row, start_col): # 한 에피소드를 실행하는 함수\n",
    "        state_row, state_col = start_row, start_col\n",
    "        path = [[start_row, start_col]]\n",
    "        while not self._done(state_row, state_col):\n",
    "            action = self._get_eps_greedy_action(state_row, state_col, self.epsilon)\n",
    "            next_state_row, next_state_col, reward = self._get_step_reward(state_row, state_col, action)\n",
    "            \n",
    "            if self.mode==\"Q-Learning\":\n",
    "                self._update_Q(state_row, state_col, action, reward, next_state_row, next_state_col)\n",
    "            elif self.mode==\"Sarsa\":\n",
    "                next_action = self._get_eps_greedy_action(next_state_row, next_state_col, self.epsilon)\n",
    "                self._update_Q(state_row, state_col, action, reward, next_state_row, next_state_col, next_action)\n",
    "\n",
    "            state_row, state_col = next_state_row, next_state_col \n",
    "            path.append([state_row, state_col])\n",
    "        #print(len(path))\n",
    "        return path\n",
    "    \n",
    "    def show_maze(self, optimal_path): # 환경을 보여주는 함수\n",
    "        maze_frame = self.maze_frame.copy() \n",
    "        for idx, [path_row, path_column] in enumerate(optimal_path):\n",
    "            maze_frame[path_row, path_column] = (0.9-0.5*(idx/len(optimal_path)))\n",
    "        ax = plt.gca()\n",
    "        ax.set_xticks(np.arange(0.5, 10, 1))\n",
    "        ax.set_yticks(np.arange(0.5, 10, 1))\n",
    "        ax.set_xticklabels([])\n",
    "        ax.set_yticklabels([])\n",
    "        img = plt.imshow(maze_frame, interpolation='none', cmap='gray')\n",
    "        #plt.savefig('output.png')\n",
    "        return img\n",
    "\n",
    "    def run_episode(self, start_row, start_col): #에피소드를 여러번 반복하는 함수\n",
    "        for i in range(self.episode_len):\n",
    "            path = self._episode(start_row, start_col)\n",
    "            if i%1==0:\n",
    "                self.show_maze(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "env = Maze(row=10, col=10, epsilon=0.1, gamma=0.99, lr=0.5, episode_len=200, mode=\"Q-Learning\")\n",
    "#env = Maze(row=10, col=10, epsilon=0.1, gamma=0.99, lr=0.9, episode_len=200, mode=\"Sarsa\")\n",
    "env.run_episode(start_row=0, start_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('modal')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "35079a8621b6d7fd325bc73feb27be9a1eedbbb1fa8df4355dab47e97cde9367"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
